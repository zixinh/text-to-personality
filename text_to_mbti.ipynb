{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "import torch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8675, 1)\n",
      "                                                  posts\n",
      "type                                                   \n",
      "INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
      "ENTP  'I'm finding the lack of me in these posts ver...\n",
      "INTP  'Good one  _____   https://www.youtube.com/wat...\n",
      "INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
      "ENTJ  'You're fired.|||That's another silly misconce...\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "df_text=pd.read_csv(\"../dataset/mbti_1.csv\",index_col='type', encoding='utf-8')\n",
    "print(df_text.shape)\n",
    "print(df_text[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and intj moments sportscenter not top ten plays prankswhat has been the most life changing experience in your life on repeat for most of today may the perc experience immerse you the last thing my infj friend posted on his facebook before committing suicide the next day rest in peace enfj7 sorry to hear of your distress it s only natural for a relationship to not be perfection all the time in every moment of existence try to figure the hard times as times of growth as 84389 84390 welcome and stuff game set match prozac wellbrutin at least thirty minutes of moving your legs and i don t mean moving them while sitting in your same desk chair weed in moderation maybe try edibles as a healthier alternative basically come up with three items you ve determined that each type or whichever types you want to do would more than likely use given each types cognitive functions and whatnot when left by all things in moderation sims is indeed a video game and a good one at that note a good one at that is somewhat subjective in that i am not completely promoting the death of any given sim dear enfp what were your favorite video games growing up and what are your now current favorite video games cool appears to be too late sad there s someone out there for everyone wait i thought confidence was a good thing i just cherish the time of solitude b c i revel within my inner world more whereas most other time i d be workin just enjoy the me time while you can don t worry people will always be around to yo entp ladies if you re into a complimentary personality well hey when your main social outlet is xbox live conversations and even then you verbally fatigue quickly i really dig the part from 1 46 to 2 50 because this thread requires it of me get high in backyard roast and eat marshmellows in backyard while conversing over something intellectual followed by massages and kisses for too many b s in that sentence how could you think of the b banned for watching movies in the corner with the dunces banned because health class clearly taught you nothing about peer pressure banned for a whole host of reasons two baby deer on left and right munching on a beetle in the middle 2 using their own blood two cavemen diary today s latest happenings on their designated cave diary wall 3 i see it as a pokemon world an infj society everyone becomes an optimist49142 all artists are artists because they draw it s the idea that counts in forming something of your own like a signature welcome to the robot ranks person who downed my self esteem cuz i m not an avid signature artist like herself proud banned for taking all the room under my bed ya gotta learn to share with the roaches for being too much of a thundering grumbling kind of storm yep ahh old high school music i haven t heard in ages failed a public speaking class a few years ago and i ve sort of learned what i could do better were i to be in that position again a big part of my failure was just overloading myself with too i like this person s mentality he s a confirmed intj by the way to the denver area and start a new life for myself\n"
     ]
    }
   ],
   "source": [
    "# preprocess text\n",
    "text_ls = df_text.posts.tolist()\n",
    "        \n",
    "def cleaner(text):\n",
    "    # remove URL\n",
    "    text = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', '', text, flags=re.MULTILINE) \n",
    "    #replace |||\n",
    "    text = text.replace('|||',\"\")\n",
    "    # remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]',' ',text)\n",
    "    #lowercase every character\n",
    "    text = text.lower()\n",
    "    # remove white space\n",
    "    text=re.sub('\\s+', ' ', text).strip()\n",
    "                \n",
    "    return text\n",
    "    \n",
    "texts = [cleaner(text) for text in text_ls]\n",
    "# validate preprocessing\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130463\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_count=Counter()\n",
    "for text in texts:\n",
    "    word_count.update(text.split(\" \"))\n",
    "    \n",
    "print(len(word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 131, 1320, 54993, 24, 697, 1866, 2114, 54994, 92, 74, 2, 87, 103, 1459, 289, 11, 41, 103, 26, 2218, 16, 87, 7, 366, 199, 2, 729, 289, 12334, 6, 2, 242, 114, 12, 128, 145, 576, 26, 136, 1045, 186, 5945, 2156, 2, 467, 183, 778, 11, 1341, 54995, 279, 3, 418, 7, 41, 7214, 8, 14, 89, 825, 16, 4, 240, 3, 24, 22, 3975, 45, 2, 62, 11, 221, 478, 7, 1531, 155, 3, 527, 2, 203, 246, 28, 246, 7, 2434, 28, 54996, 54997, 300, 5, 275, 492, 711, 1224, 12622, 54998, 44, 259, 6747, 1003, 7, 1085, 41, 3315, 5, 1, 37, 13, 162, 1085, 66, 171, 1114, 11, 41, 132, 2821, 3878, 1992, 11, 6971, 152, 155, 27506, 28, 4, 5946, 2224, 519, 215, 63, 18, 556, 3462, 6, 53, 2800, 9, 315, 94, 35, 6572, 223, 6, 84, 3, 36, 51, 49, 95, 409, 225, 635, 315, 223, 640, 348, 5, 4362, 42, 508, 75, 45, 80, 11, 6971, 5245, 10, 1037, 4, 463, 492, 5, 4, 79, 47, 44, 9, 937, 4, 79, 47, 44, 9, 10, 842, 1484, 11, 9, 1, 60, 24, 356, 10108, 2, 677, 7, 111, 635, 11293, 607, 172, 33, 133, 41, 420, 463, 564, 1142, 63, 5, 33, 23, 41, 96, 951, 420, 463, 564, 360, 2080, 3, 22, 72, 687, 472, 55, 14, 101, 56, 55, 16, 194, 647, 1, 137, 1136, 30, 4, 79, 114, 1, 31, 7044, 2, 62, 7, 3865, 919, 982, 1, 13751, 820, 12, 1191, 187, 49, 2571, 87, 78, 62, 1, 93, 22, 25242, 31, 312, 2, 19, 62, 171, 6, 34, 37, 13, 800, 40, 88, 99, 22, 151, 3, 2683, 210, 2772, 32, 6, 65, 115, 4, 11788, 243, 73, 401, 42, 41, 794, 302, 5351, 10, 8231, 308, 934, 5, 98, 91, 6, 3417, 9758, 996, 1, 50, 3110, 2, 249, 59, 150, 5726, 3, 157, 1035, 58, 21, 153, 2126, 8, 7, 19, 57, 276, 11, 7944, 10109, 5, 665, 27507, 11, 7944, 171, 7677, 143, 83, 1316, 2047, 75, 16653, 5, 5107, 16, 72, 140, 919, 14, 11, 9, 1629, 54, 105, 6, 39, 7, 2, 919, 2737, 16, 539, 708, 11, 2, 2176, 18, 2, 41400, 2737, 58, 1285, 569, 940, 1739, 6, 273, 38, 5505, 1742, 2737, 16, 4, 349, 5144, 7, 669, 178, 1206, 7851, 26, 508, 5, 122, 30470, 26, 4, 19758, 11, 2, 796, 157, 362, 104, 198, 1563, 178, 17962, 4927, 366, 14, 3079, 15526, 26, 104, 11542, 4056, 4927, 1406, 197, 1, 85, 8, 28, 4, 3178, 187, 27, 128, 726, 194, 1443, 27, 54999, 45, 2402, 23, 2402, 58, 43, 1253, 8, 14, 2, 252, 9, 2801, 11, 5586, 83, 7, 41, 198, 25, 4, 2127, 300, 3, 2, 2479, 8232, 120, 68, 30471, 12, 267, 1785, 2264, 1, 20, 24, 27, 7771, 2127, 1591, 25, 1749, 621, 2737, 16, 453, 45, 2, 586, 617, 12, 889, 1130, 1410, 432, 3, 500, 18, 2, 18814, 16, 71, 72, 70, 7, 4, 34696, 34697, 169, 7, 3976, 1082, 2894, 316, 276, 220, 278, 1, 333, 13, 498, 11, 2414, 1861, 4, 942, 775, 569, 4, 175, 161, 323, 5, 1, 53, 371, 7, 681, 33, 1, 105, 36, 177, 133, 1, 3, 22, 11, 9, 1214, 277, 4, 326, 249, 7, 12, 2563, 30, 31, 25243, 106, 18, 72, 1, 25, 21, 120, 14, 3097, 52, 14, 4, 2705, 131, 75, 2, 82, 3, 2, 11789, 1193, 5, 294, 4, 212, 103, 16, 106]\n"
     ]
    }
   ],
   "source": [
    "# create a vocabulary dictionary and transfer words to word representation\n",
    "vocab = sorted(word_count, key=word_count.get, reverse=True)\n",
    "vocab_to_int = {word: num for num, word in enumerate(vocab, 1)}\n",
    "\n",
    "text_rep = []\n",
    "for text in texts:\n",
    "    text_rep.append([vocab_to_int[word] for word in text.split()])\n",
    "    \n",
    "print(text_rep[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1311.4900288184438\n"
     ]
    }
   ],
   "source": [
    "# since each data has different length of words, need to uniform all feature length\n",
    "lengths = [len(rep) for rep in text_rep]\n",
    "print(sum(lengths)/len(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1  20 761 ...,   1 117  17]\n",
      "(8675, 1200)\n"
     ]
    }
   ],
   "source": [
    "# let's set the length as 1200\n",
    "sample_len = 1200\n",
    "features =np.zeros((len(text_rep),sample_len),dtype=int)\n",
    "for i, sample in enumerate(text_rep):\n",
    "    # fill 0 for samples has length < 1200\n",
    "    features[i, -len(sample):] = np.array(sample)[:sample_len]\n",
    "print(features[1])\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#one-hot code labels\n",
    "labels=df_text.index.tolist()\n",
    "encoder=LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)\n",
    "labels=encoder.fit_transform(labels)\n",
    "labels=np.array(labels)\n",
    "print(labels[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6940, 1200)\n",
      "(6940, 16)\n",
      "(867, 1200)\n",
      "(867, 16)\n",
      "(868, 1200)\n",
      "(868, 16)\n"
     ]
    }
   ],
   "source": [
    "# Split Dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state = 1)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state = 1)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 22.92849719\n",
      "Iteration 2, loss = 10.74641705\n",
      "Iteration 3, loss = 10.51665288\n",
      "Iteration 4, loss = 10.29333402\n",
      "Iteration 5, loss = 10.07656054\n",
      "Iteration 6, loss = 9.86627384\n",
      "0.0818915801615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zixinhuang/anaconda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "# sklearn sklearn MLP classifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(512,512,512,256,16), verbose=True, random_state=1)\n",
    "mlp.fit(x_train, y_train)\n",
    "acc = mlp.score(x_test, y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size=100):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "# reference: http://deeplearningathome.com/2017/06/PyTorch-vs-Tensorflow-lstm-language-model.html \n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_steps, batch_size, vocab_size, num_layers, dp_keep_prob):\n",
    "        super(LM_LSTM, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_steps = num_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dp_keep_prob = dp_keep_prob\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(1 - dp_keep_prob)\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=embedding_dim,\n",
    "                            num_layers=num_layers,\n",
    "                            dropout=1 - dp_keep_prob)\n",
    "        self.sm_fc = nn.Linear(in_features=embedding_dim,\n",
    "                               out_features=vocab_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_range = 0.1\n",
    "        self.word_embeddings.weight.data.uniform_(-init_range, init_range)\n",
    "        self.sm_fc.bias.data.fill_(0.0)\n",
    "        self.sm_fc.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (Variable(weight.new(self.num_layers, self.batch_size, self.embedding_dim).zero_()),\n",
    "                Variable(weight.new(self.num_layers, self.batch_size, self.embedding_dim).zero_()))\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        embeds = self.dropout(self.word_embeddings(inputs))\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        logits = self.sm_fc(lstm_out.view(-1, self.embedding_dim))\n",
    "        return logits.view(self.num_steps, self.batch_size, self.vocab_size), hidden\n",
    "\n",
    "    def repackage_hidden(h):\n",
    "        if type(h) == Variable:\n",
    "            return Variable(h.data)\n",
    "        else:\n",
    "            return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-31f3b19f3c76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zixinhuang/anaconda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-f93c6ede5964>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zixinhuang/anaconda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zixinhuang/anaconda/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zixinhuang/anaconda/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "# LSTM RNN\n",
    "from torch import optim\n",
    "\n",
    "n_epochs = 3\n",
    "hidden_size = 256\n",
    "batch_size = 50\n",
    "iteration = 50\n",
    "embedding_dim = 1200\n",
    "num_steps = 35\n",
    "vocab_size = 130463\n",
    "num_layers = 1\n",
    "dp_keep_prob = 0.35\n",
    "\n",
    "\n",
    "model = LSTM(embedding_dim, num_steps, batch_size, vocab_size, num_layers, dp_keep_prob)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    #for iter in range(n_iters):\n",
    "    for i, (x_batch, y_batch) in enumerate(get_batches(x_train, y_train, batch_size),1):\n",
    "        \n",
    "        inputs = Variable(torch.from_numpy(x_batch).float())\n",
    "        targets = Variable(torch.from_numpy(y_batch).float())\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if iteration%5==0:\n",
    "            print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                  \"Iteration: {}\".format(iteration),\n",
    "                  \"Train loss: {:.3f}\".format(loss))\n",
    "            \n",
    "        if iteration %25 == 0:\n",
    "            val_acc = []\n",
    "            for val_x, val_y in get_batches(x_val, y_val, batch_size):\n",
    "                \n",
    "                inputs_val = Variable(torch.from_numpy(val_x))\n",
    "                targets_val = Variable(torch.from_numpy(val_y))\n",
    "    \n",
    "                outputs_val, hidden = model(inputs_val, batch_size)\n",
    "        \n",
    "                batch_acc = (outputs_val == targets_val).sum()/batch_size\n",
    "                val_acc.append(batch_acc)\n",
    "            print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "\n",
    "        iteration+=1\n",
    "        \n",
    "    if epoch > 0:\n",
    "        print(epoch, loss.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testing\n",
    "test_acc = []\n",
    "for test_x, test_y in get_batches(x_test, y_test, batch_size):\n",
    "                \n",
    "    inputs_test = Variable(torch.from_numpy(test_x))\n",
    "    targets_test = Variable(torch.from_numpy(test_y))\n",
    "    \n",
    "    outputs_test, hidden = model(inputs_test, batch_size)\n",
    "        \n",
    "    test_acc = (outputs_test == targets_test).sum()/batch_size\n",
    "    test_acc.append(batch_acc)\n",
    "print(\"Test acc: {:.3f}\".format(np.mean(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. pure word count\n",
    "# 2. if 1. doesn't work, try glovec\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
