{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "import torch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8675, 1)\n",
      "                                                  posts\n",
      "type                                                   \n",
      "INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
      "ENTP  'I'm finding the lack of me in these posts ver...\n",
      "INTP  'Good one  _____   https://www.youtube.com/wat...\n",
      "INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
      "ENTJ  'You're fired.|||That's another silly misconce...\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "df_text=pd.read_csv(\"../dataset/mbti_1.csv\",index_col='type', encoding='utf-8')\n",
    "print(df_text.shape)\n",
    "print(df_text[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and intj moments sportscenter not top ten plays prankswhat has been the most life changing experience in your life on repeat for most of today may the perc experience immerse you the last thing my infj friend posted on his facebook before committing suicide the next day rest in peace enfj7 sorry to hear of your distress it s only natural for a relationship to not be perfection all the time in every moment of existence try to figure the hard times as times of growth as 84389 84390 welcome and stuff game set match prozac wellbrutin at least thirty minutes of moving your legs and i don t mean moving them while sitting in your same desk chair weed in moderation maybe try edibles as a healthier alternative basically come up with three items you ve determined that each type or whichever types you want to do would more than likely use given each types cognitive functions and whatnot when left by all things in moderation sims is indeed a video game and a good one at that note a good one at that is somewhat subjective in that i am not completely promoting the death of any given sim dear enfp what were your favorite video games growing up and what are your now current favorite video games cool appears to be too late sad there s someone out there for everyone wait i thought confidence was a good thing i just cherish the time of solitude b c i revel within my inner world more whereas most other time i d be workin just enjoy the me time while you can don t worry people will always be around to yo entp ladies if you re into a complimentary personality well hey when your main social outlet is xbox live conversations and even then you verbally fatigue quickly i really dig the part from 1 46 to 2 50 because this thread requires it of me get high in backyard roast and eat marshmellows in backyard while conversing over something intellectual followed by massages and kisses for too many b s in that sentence how could you think of the b banned for watching movies in the corner with the dunces banned because health class clearly taught you nothing about peer pressure banned for a whole host of reasons two baby deer on left and right munching on a beetle in the middle 2 using their own blood two cavemen diary today s latest happenings on their designated cave diary wall 3 i see it as a pokemon world an infj society everyone becomes an optimist49142 all artists are artists because they draw it s the idea that counts in forming something of your own like a signature welcome to the robot ranks person who downed my self esteem cuz i m not an avid signature artist like herself proud banned for taking all the room under my bed ya gotta learn to share with the roaches for being too much of a thundering grumbling kind of storm yep ahh old high school music i haven t heard in ages failed a public speaking class a few years ago and i ve sort of learned what i could do better were i to be in that position again a big part of my failure was just overloading myself with too i like this person s mentality he s a confirmed intj by the way to the denver area and start a new life for myself\n"
     ]
    }
   ],
   "source": [
    "# preprocess text\n",
    "text_ls = df_text.posts.tolist()\n",
    "        \n",
    "def cleaner(text):\n",
    "    # remove URL\n",
    "    text = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', '', text, flags=re.MULTILINE) \n",
    "    #replace |||\n",
    "    text = text.replace('|||',\"\")\n",
    "    # remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]',' ',text)\n",
    "    #lowercase every character\n",
    "    text = text.lower()\n",
    "    # remove white space\n",
    "    text=re.sub('\\s+', ' ', text).strip()\n",
    "                \n",
    "    return text\n",
    "    \n",
    "texts = [cleaner(text) for text in text_ls]\n",
    "# validate preprocessing\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130463\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_count=Counter()\n",
    "for text in texts:\n",
    "    word_count.update(text.split(\" \"))\n",
    "    \n",
    "print(len(word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 131, 1320, 54993, 24, 697, 1866, 2114, 54994, 92, 74, 2, 87, 103, 1459, 289, 11, 41, 103, 26, 2218, 16, 87, 7, 366, 199, 2, 729, 289, 12334, 6, 2, 242, 114, 12, 128, 145, 576, 26, 136, 1045, 186, 5945, 2156, 2, 467, 183, 778, 11, 1341, 54995, 279, 3, 418, 7, 41, 7214, 8, 14, 89, 825, 16, 4, 240, 3, 24, 22, 3975, 45, 2, 62, 11, 221, 478, 7, 1531, 155, 3, 527, 2, 203, 246, 28, 246, 7, 2434, 28, 54996, 54997, 300, 5, 275, 492, 711, 1224, 12622, 54998, 44, 259, 6747, 1003, 7, 1085, 41, 3315, 5, 1, 37, 13, 162, 1085, 66, 171, 1114, 11, 41, 132, 2821, 3878, 1992, 11, 6971, 152, 155, 27506, 28, 4, 5946, 2224, 519, 215, 63, 18, 556, 3462, 6, 53, 2800, 9, 315, 94, 35, 6572, 223, 6, 84, 3, 36, 51, 49, 95, 409, 225, 635, 315, 223, 640, 348, 5, 4362, 42, 508, 75, 45, 80, 11, 6971, 5245, 10, 1037, 4, 463, 492, 5, 4, 79, 47, 44, 9, 937, 4, 79, 47, 44, 9, 10, 842, 1484, 11, 9, 1, 60, 24, 356, 10108, 2, 677, 7, 111, 635, 11293, 607, 172, 33, 133, 41, 420, 463, 564, 1142, 63, 5, 33, 23, 41, 96, 951, 420, 463, 564, 360, 2080, 3, 22, 72, 687, 472, 55, 14, 101, 56, 55, 16, 194, 647, 1, 137, 1136, 30, 4, 79, 114, 1, 31, 7044, 2, 62, 7, 3865, 919, 982, 1, 13751, 820, 12, 1191, 187, 49, 2571, 87, 78, 62, 1, 93, 22, 25242, 31, 312, 2, 19, 62, 171, 6, 34, 37, 13, 800, 40, 88, 99, 22, 151, 3, 2683, 210, 2772, 32, 6, 65, 115, 4, 11788, 243, 73, 401, 42, 41, 794, 302, 5351, 10, 8231, 308, 934, 5, 98, 91, 6, 3417, 9758, 996, 1, 50, 3110, 2, 249, 59, 150, 5726, 3, 157, 1035, 58, 21, 153, 2126, 8, 7, 19, 57, 276, 11, 7944, 10109, 5, 665, 27507, 11, 7944, 171, 7677, 143, 83, 1316, 2047, 75, 16653, 5, 5107, 16, 72, 140, 919, 14, 11, 9, 1629, 54, 105, 6, 39, 7, 2, 919, 2737, 16, 539, 708, 11, 2, 2176, 18, 2, 41400, 2737, 58, 1285, 569, 940, 1739, 6, 273, 38, 5505, 1742, 2737, 16, 4, 349, 5144, 7, 669, 178, 1206, 7851, 26, 508, 5, 122, 30470, 26, 4, 19758, 11, 2, 796, 157, 362, 104, 198, 1563, 178, 17962, 4927, 366, 14, 3079, 15526, 26, 104, 11542, 4056, 4927, 1406, 197, 1, 85, 8, 28, 4, 3178, 187, 27, 128, 726, 194, 1443, 27, 54999, 45, 2402, 23, 2402, 58, 43, 1253, 8, 14, 2, 252, 9, 2801, 11, 5586, 83, 7, 41, 198, 25, 4, 2127, 300, 3, 2, 2479, 8232, 120, 68, 30471, 12, 267, 1785, 2264, 1, 20, 24, 27, 7771, 2127, 1591, 25, 1749, 621, 2737, 16, 453, 45, 2, 586, 617, 12, 889, 1130, 1410, 432, 3, 500, 18, 2, 18814, 16, 71, 72, 70, 7, 4, 34696, 34697, 169, 7, 3976, 1082, 2894, 316, 276, 220, 278, 1, 333, 13, 498, 11, 2414, 1861, 4, 942, 775, 569, 4, 175, 161, 323, 5, 1, 53, 371, 7, 681, 33, 1, 105, 36, 177, 133, 1, 3, 22, 11, 9, 1214, 277, 4, 326, 249, 7, 12, 2563, 30, 31, 25243, 106, 18, 72, 1, 25, 21, 120, 14, 3097, 52, 14, 4, 2705, 131, 75, 2, 82, 3, 2, 11789, 1193, 5, 294, 4, 212, 103, 16, 106]\n"
     ]
    }
   ],
   "source": [
    "# create a vocabulary dictionary and transfer words to word representation\n",
    "vocab = sorted(word_count, key=word_count.get, reverse=True)\n",
    "vocab_to_int = {word: num for num, word in enumerate(vocab, 1)}\n",
    "\n",
    "text_rep = []\n",
    "for text in texts:\n",
    "    text_rep.append([vocab_to_int[word] for word in text.split()])\n",
    "    \n",
    "print(text_rep[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1311.4900288184438\n"
     ]
    }
   ],
   "source": [
    "# since each data has different length of words, need to uniform all feature length\n",
    "lengths = [len(rep) for rep in text_rep]\n",
    "print(sum(lengths)/len(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    1    20   761     2   636     7    19    11   180   520    64 17963\n",
      "   494    34    22   766    32     8    14    11     2   132  1214   237\n",
      "    16   396    19     5    12   979    23   603    11    27  1119   139\n",
      "    46    17     3  7604   225 30472     5 16064    55   257    13   250\n",
      "   721   212   713     3   492   582   510   210  5178     9    14    45\n",
      "     8   652    95    46  5145     5    43    36    87     7     2  1730\n",
      "   171     1  2989   104  2403     5  1906   104   364    18  4017 12335\n",
      "     5    49  9467 15023    21   636     7  1492     5   634   767  8134\n",
      "   301  1412   306     1  1096 16065   611  1412   724    23   389     1\n",
      "  1096 41401    35  1245    96    25     2  2328  1115     7    21   153\n",
      "     1    88   992     9     1    37    13   227    11     2  1412   306\n",
      "   186     6 21974     6    48     6    65    27   210    42     6 11790\n",
      "    59     4   848    16     4   268     5     4   584  1906     5   113\n",
      "    40    23   134  5762    26    41   520     5  1892    41   485   383\n",
      "     6    48     6    65    27   210    42     6   143    39    80   147\n",
      "     1   110    75     2   316  2444  4317   734   488    42     4   317\n",
      "    92   867   807     5   867  2790    25    12   198     8   298 10110\n",
      "   125     3  1402     4  1281 55000  2538  1837    29    10     1 55001\n",
      "  2526 34698    50     1    53   107   137     7   470     1    35   685\n",
      "   327    28   301   348     1  1379   106    26    33     1   225     1\n",
      "   225   329     5   367    28    12 12336   284    16   393     5   880\n",
      "   481     1    90   225   334   633     3    19  1940     6    48   112\n",
      "     9    30 15024   174   307     8     1    50    84     3   155     8\n",
      "     5    85    33   533    18    19   596     4   123   120  9619    11\n",
      "     2   158   171    46   976   151     1    84     3    85     2   179\n",
      "    26    56     7    45     7    66     2  1106  1393    47    10     2\n",
      "   154     8   190    19   201     6   244    23  1573    93     1    20\n",
      "    50   276    63    26     2  2538   747    29   142     6   418    38\n",
      "     9   212   123   120  9619   492     1    53    74 10668     2   597\n",
      "    56     7     2  4500    26    12  4958   414  8333     9    88  3208\n",
      "     2  8436    46  1960     3   264     4   454  9174    14    11    67\n",
      "     2    82    52  1936    80    30    64   329   329 12336    23    31\n",
      "    28   715     7   104  5017    28   413 12336   396 15527  9175    35\n",
      "  6150  3269   202   622    73  4138     1    88    22     2   123     3\n",
      "   980     1    36    57  1863    25     6    36     1  8918     8    63\n",
      "     3    12  2115   523  2002    18    12  4318  2349  6406     5  5634\n",
      "   202    25     3    22   595   297    14    25     3    22   615    24\n",
      "     2   132    93     1   117  5727     2   132  4605    18     2  8686\n",
      "   314    59    12  1567    95     6   402    13   418   163  2415  7454\n",
      "   846    15    18 23437 41402 34699    10     4    50   216   443    28\n",
      "   173    28     6    34   826  2914    56     2  3308     1    76     2\n",
      "  1441     8   190    19  5074  1714 55002 55003]\n",
      "(8675, 500)\n"
     ]
    }
   ],
   "source": [
    "# let's set the length as 500\n",
    "sample_len = 500\n",
    "features =np.zeros((len(text_rep),sample_len),dtype=int)\n",
    "for i, sample in enumerate(text_rep):\n",
    "    # fill 0 for samples has length < 1200\n",
    "    features[i, -len(sample):] = np.array(sample)[:sample_len]\n",
    "print(features[1])\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#one-hot code labels\n",
    "labels=df_text.index.tolist()\n",
    "encoder=LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)\n",
    "labels=encoder.fit_transform(labels)\n",
    "labels=np.array(labels)\n",
    "print(labels[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6940, 500)\n",
      "(6940, 16)\n",
      "(867, 500)\n",
      "(867, 16)\n",
      "(868, 500)\n",
      "(868, 16)\n"
     ]
    }
   ],
   "source": [
    "# Split Dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state = 1)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state = 1)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sklearn sklearn MLP classifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(512,512,512,256,16), verbose=True, random_state=1)\n",
    "#mlp.fit(x_train, y_train)\n",
    "#acc = mlp.score(x_test, y_test)\n",
    "#print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size=100):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "# reference: http://deeplearningathome.com/2017/06/PyTorch-vs-Tensorflow-lstm-language-model.html \n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, batch_size, vocab_size, num_layers, dp_keep_prob, out_dim, sample_len):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        #self.num_steps = num_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.out_dim = out_dim\n",
    "        self.dp_keep_prob = dp_keep_prob\n",
    "        self.num_layers = num_layers\n",
    "        self.sample_len = sample_len\n",
    "\n",
    "        self.dropout = nn.Dropout(1 - dp_keep_prob)\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=embedding_dim,\n",
    "                            num_layers=num_layers,\n",
    "                            dropout=1 - dp_keep_prob)\n",
    "        self.sm_fc = nn.Linear(in_features=embedding_dim*sample_len,\n",
    "                               out_features=out_dim)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_range = 0.1\n",
    "        self.word_embeddings.weight.data.uniform_(-init_range, init_range)\n",
    "        self.sm_fc.bias.data.fill_(0.0)\n",
    "        self.sm_fc.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (Variable(weight.new(self.num_layers, self.batch_size, self.embedding_dim).zero_()),\n",
    "                Variable(weight.new(self.num_layers, self.batch_size, self.embedding_dim).zero_()))\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        embeds = self.word_embeddings(inputs)\n",
    "        embeds = self.dropout(embeds)\n",
    "        embeds = embeds.view(self.sample_len,self.batch_size, -1)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        logits = self.sm_fc(lstm_out.transpose(0,1).contiguous().view(self.batch_size, -1))\n",
    "        out = self.softmax(logits.view(self.batch_size, self.out_dim))\n",
    "        return out, hidden\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    # use for multiple hidden layer\n",
    "    if type(h) == Variable:\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zixinhuang/anaconda/lib/python3.6/site-packages/torch/autograd/__init__.py:92: UserWarning: retain_variables option is deprecated and will be removed in 0.3. Use retain_graph instead.\n",
      "  warnings.warn(\"retain_variables option is deprecated and will be removed in 0.3. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/1 Iteration: 1 Train loss: 0.062\n",
      "Epoch: 0/1 Iteration: 2 Train loss: 0.059\n",
      "Epoch: 0/1 Iteration: 3 Train loss: 0.057\n",
      "Epoch: 0/1 Iteration: 4 Train loss: 0.056\n",
      "Epoch: 0/1 Iteration: 5 Train loss: 0.057\n",
      "Val acc: 14.398\n"
     ]
    }
   ],
   "source": [
    "# LSTM RNN\n",
    "from torch import optim\n",
    "\n",
    "n_epochs = 1\n",
    "hidden_size = 256\n",
    "batch_size = 100\n",
    "iteration = 1\n",
    "embedding_dim = 250\n",
    "vocab_size = 130463\n",
    "num_layers = 1\n",
    "dp_keep_prob = 0.35\n",
    "out_dim = 16\n",
    "sample_len = 500\n",
    "\n",
    "model = LSTM(embedding_dim, batch_size, vocab_size, num_layers, dp_keep_prob, out_dim, sample_len)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.01, weight_decay=0.04)\n",
    "stop_loss = 10\n",
    "\n",
    "for epoch in range(n_epochs):        \n",
    "    hidden = model.init_hidden()\n",
    "    hidden = repackage_hidden(hidden)\n",
    "    \n",
    "    for i, (x_batch, y_batch) in enumerate(get_batches(x_train, y_train, batch_size),1):\n",
    "        # set training mode\n",
    "        model.train()\n",
    "\n",
    "        inputs = Variable(torch.from_numpy(x_batch.astype(np.int64)))\n",
    "        targets = Variable(torch.from_numpy(y_batch).float())\n",
    "        \n",
    "        outputs,hidden = model(inputs, hidden)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward(retain_variables = True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (iteration%1)==0:\n",
    "            print(\"Epoch: {}/{}\".format(epoch, n_epochs),\n",
    "                  \"Iteration: {}\".format(iteration),\n",
    "                  \"Train loss: {:.3f}\".format(loss.data[0]))\n",
    "            \n",
    "            if loss.data[0] < stop_loss:\n",
    "                stop_loss = loss.data[0]\n",
    "            elif (stop_loss - loss.data[0]) > 0.01:\n",
    "                print(\"loss larger than previous iteration, stop at loss:{}, best loss is:{}\"\n",
    "                      .format(loss.data[0], stop_loss)) \n",
    "                break\n",
    "            \n",
    "        if (iteration%5)==0:\n",
    "            val_acc = []\n",
    "            for val_x, val_y in get_batches(x_val, y_val, batch_size):\n",
    "                \n",
    "                inputs_val = Variable(torch.from_numpy(val_x.astype(np.int64)))\n",
    "                targets_val = val_y\n",
    "                \n",
    "                # set eval mode\n",
    "                model.eval()\n",
    "                outputs_val, hidden = model(inputs_val, hidden)\n",
    "                # transform output to one-hot\n",
    "                outputs_val = outputs_val.data.numpy()\n",
    "                outputs_val = (outputs_val == outputs_val.max(axis=1, keepdims=True)).astype(int)\n",
    "                batch_acc = (outputs_val == targets_val).sum()/batch_size\n",
    "                val_acc.append(batch_acc)    \n",
    "            print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "            if np.mean(val_acc) > 90:\n",
    "                break\n",
    "\n",
    "        iteration+=1\n",
    "        \n",
    "    if epoch > 0:\n",
    "        print(epoch, loss.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 15.000\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "test_acc = []\n",
    "for test_x, test_y in get_batches(x_test, y_test, batch_size):\n",
    "                \n",
    "    inputs_test = Variable(torch.from_numpy(test_x))\n",
    "    targets_test = test_y\n",
    "    \n",
    "    model.eval()\n",
    "    outputs_test, hidden = model(inputs_test, hidden)\n",
    "    outputs_test = outputs_test.round()\n",
    "    outputs_test = outputs_test.data.numpy().astype(np.int64)\n",
    "        \n",
    "    batch_acc = (outputs_test == targets_test).sum()/batch_size\n",
    "    test_acc.append(batch_acc)\n",
    "print(\"Test acc: {:.3f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
